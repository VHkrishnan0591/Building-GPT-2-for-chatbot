{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total length of the filename is: 20479\n"
     ]
    }
   ],
   "source": [
    "# Opening the file\n",
    "import re\n",
    "filename = 'the-verdict.txt'\n",
    "with open(filename,'r') as f:\n",
    "    data = f.read()\n",
    "print(\"The total length of the filename is:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The match is: ['Hello', ',', '', ' ', 'world', '.', '', ' ', 'Is', ' ', 'this', '--', '', ' ', 'a', ' ', 'test', '?', '']\n",
      "['', 'test', ' ', 'this', 'Hello', 'Is', ',', '--', 'world', '.', '?', 'a']\n",
      "['test', 'this', 'Hello', 'Is', ',', '--', 'world', '.', '?', 'a']\n"
     ]
    }
   ],
   "source": [
    "# text = \"Hello, world. This, is a test.\"\n",
    "text = \"Hello, world. Is this-- a test?\"\n",
    "# [,.:;?_!\"()\\']\n",
    "match = re.split(r'([],.:;?_!\"-()\\']|--|\\s+)', text) \n",
    "if match:\n",
    "    print(\"The match is:\" ,match)\n",
    "match = list(set(match))\n",
    "print(match)\n",
    "match.remove('')\n",
    "match.remove(' ')\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1131\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([],.:;?_!\"-()\\']|--|\\s+)', data) \n",
    "result = sorted(list(set(result)))\n",
    "result.remove('')\n",
    "result.remove(' ')\n",
    "result.remove('\\n\\n')\n",
    "result.append(\"<|unk|>\")\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab ={}\n",
    "for i in range(len(result)):\n",
    "    vocab[i] = result[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "palace\n",
      "[55, 988, 956, 984, 722, 988, 1131, 7]\n",
      "In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "class SimpleTokeniser():\n",
    "    def __init__(self):\n",
    "        self.vocab={}\n",
    "        filename = 'the-verdict.txt'\n",
    "        with open(filename,'r') as f:\n",
    "            data = f.read()\n",
    "        result = re.split(r'([],.:;?_!\"-()\\']|--|\\s+)', data) \n",
    "        result = sorted(list(set(result)))\n",
    "        result.remove('')\n",
    "        result.remove(' ')\n",
    "        result.remove('\\n\\n')\n",
    "        result.extend([\"<|endoftext|>\",\"<|unk|>\"])\n",
    "        for i in range(len(result)):\n",
    "            self.vocab[result[i]] = i\n",
    "    def encode(self,data):\n",
    "        result = re.split(r'([],.:;?_!\"-()\\']|--|\\s+)', data) \n",
    "        result = [item.strip() for item in result if item.strip()]\n",
    "        token = []\n",
    "        for i in result:\n",
    "            if i in self.vocab:\n",
    "                # print(i)\n",
    "                token.append(int(self.vocab[i]))\n",
    "                # print(self.vocab[i])\n",
    "            else:\n",
    "                print(i)\n",
    "                token.append(int(self.vocab[\"<|unk|>\"]))\n",
    "        return token\n",
    "    def decode(self, token):\n",
    "        result = \" \"\n",
    "        decode_vocab = dict((v,k) for k,v in self.vocab.items())\n",
    "        for i in token:\n",
    "            if i in decode_vocab:\n",
    "                if decode_vocab[i] in [']',',','.',':',';','?','_','!','\"','-','(',')','\\\\']:\n",
    "                    result = result + decode_vocab[i]\n",
    "                else:\n",
    "                    result = result + \" \" + decode_vocab[i]\n",
    "        return result.strip()\n",
    "        # return result\n",
    "        \n",
    "tokenizer = SimpleTokeniser()\n",
    "result = tokenizer.encode(\"In the sunlit terraces of the palace.\")\n",
    "print(result)\n",
    "decoder = tokenizer.decode([55, 988, 956, 984, 722, 988, 1131, 7])\n",
    "print(decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.9.0\n",
      "5145\n"
     ]
    }
   ],
   "source": [
    "# Byte encoding\n",
    "\n",
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Encode the given text\n",
    "enc_text = tokenizer.encode(data)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating data loaders\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, tokenizer, data, stride, context_length):\n",
    "        self.input =[]\n",
    "        self.output =[]\n",
    "        self.tokenizer = tokenizer\n",
    "        enc_text = tokenizer.encode(data)\n",
    "        for i in range(0,len(enc_text)-context_length, stride):\n",
    "            self.input.append(enc_text[i:i+context_length])\n",
    "            self.output.append(enc_text[i+1:i+1+context_length])\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "    def __getitem__(self,index):\n",
    "        return torch.tensor(self.input)[index], torch.tensor(self.output)[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataloader object\n",
    "\n",
    "def create_dataloader(data, context_length=256, stride=128,shuffle= False,batch_size=4,num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    enc_text = tokenizer.encode(data)\n",
    "    dataset = SimpleDataset(tokenizer, data, stride, context_length)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle,num_workers=num_workers)\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "# Initiating the dataloader object\n",
    "dataloader = create_dataloader(\n",
    "    data, context_length=4, stride=1, shuffle=False, batch_size=1\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initating the token and positional embeddings layer\n",
    "\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "context_length=4\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "positional_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the input torch.Size([8, 4])\n",
      "torch.Size([8, 4, 256])\n",
      "torch.Size([4, 256])\n",
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "# Intialising the first batch of input\n",
    "dataloader = create_dataloader(\n",
    "    data, context_length=4, stride=1, shuffle=False, batch_size=8\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "input,output = next(data_iter)\n",
    "print(\"The shape of the input\", input.shape)\n",
    "\n",
    "# Initalise the token and postional embedding\n",
    "\n",
    "token_embeddings = token_embedding_layer(input)\n",
    "print(token_embeddings.shape)\n",
    "positonal_embedding = positional_embedding_layer(torch.arange(context_length))\n",
    "print(positonal_embedding.shape)\n",
    "\n",
    "# Intiating the input embedding containing the both positional and token embeddings\n",
    "input_embeddings = token_embeddings + positonal_embedding\n",
    "print(input_embeddings.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
