{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELF ATTENTION MECHANISM(WITHOUT TRAINABLE WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as f\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalizing the attention weights\n",
    "attention_weights = torch.matmul(inputs, inputs.T)\n",
    "attention_weights = f.softmax(attention_weights)\n",
    "# Context Vectors \n",
    "context_vectors = torch.matmul(attention_weights, inputs)\n",
    "context_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELF ATTENTION MECHANISM WITH TRAINABLE WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harik\\AppData\\Local\\Temp\\ipykernel_65240\\360204716.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attention_scores1 = f.softmax(attention_scores1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0370, -0.6304,  0.1504],\n",
       "        [-0.0276, -0.6395,  0.1476],\n",
       "        [-0.0221, -0.6392,  0.1432],\n",
       "        [ 0.0421, -0.6363,  0.0952],\n",
       "        [ 0.1059, -0.6293,  0.0442],\n",
       "        [-0.0144, -0.6396,  0.1394]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_of_weight_matrix = inputs.shape[-1]\n",
    "\n",
    "# Initialize weights and queries of the query, value and key matrices\n",
    "torch.manual_seed(123)\n",
    "weights_query = torch.nn.Parameter(torch.randn(dim_of_weight_matrix, dim_of_weight_matrix)) \n",
    "# We can use nn.Linear instead of nn.parameter because they can add bias and good sophisicated weight inititalisation scheme\n",
    "weights_value = torch.nn.Parameter(torch.randn(dim_of_weight_matrix, dim_of_weight_matrix))\n",
    "weights_key = torch.nn.Parameter(torch.randn(dim_of_weight_matrix, dim_of_weight_matrix))\n",
    "\n",
    "# Initialize queries, values and keys\n",
    "query = torch.matmul(inputs,weights_query)\n",
    "key = torch.matmul(inputs,weights_key)\n",
    "value = torch.matmul(inputs,weights_value)\n",
    "\n",
    "# Calculate the attention scores\n",
    "attention_scores = torch.matmul(query,key.T)\n",
    "attention_scores1 = attention_scores/key.shape[-1]**0.5\n",
    "attention_scores1 = f.softmax(attention_scores1)\n",
    "\n",
    "# Calculate the context vectors\n",
    "context_vectors = torch.matmul(attention_scores1,key)\n",
    "context_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MASKED SELF ATTENTION WITH DROPOUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "tensor([[ 0.0000,  0.0000,  0.0000],\n",
      "        [-0.5905,  0.2512, -0.3913],\n",
      "        [-1.1032,  0.5765, -0.6179],\n",
      "        [-0.4307,  0.1662, -0.2997],\n",
      "        [-0.5427,  0.3251, -0.2797],\n",
      "        [-0.6820,  0.3368, -0.4002]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dim_of_weight_matrix = inputs.shape[-1]\n",
    "print(dim_of_weight_matrix)\n",
    "torch.seed()\n",
    "weights_query = torch.nn.Linear(dim_of_weight_matrix, dim_of_weight_matrix,bias = False)\n",
    "weights_value = torch.nn.Linear(dim_of_weight_matrix, dim_of_weight_matrix,bias = False)\n",
    "weights_key = torch.nn.Linear(dim_of_weight_matrix, dim_of_weight_matrix,bias = False)\n",
    "query = weights_query(inputs)\n",
    "key = weights_key(inputs)\n",
    "value = weights_value(inputs)\n",
    "attention_scores = torch.matmul(query,key.T)\n",
    "# print(attention_scores)\n",
    "mask = torch.triu(torch.ones(attention_scores.shape[0], attention_scores.shape[0]),diagonal=1) ==1\n",
    "# print(mask)\n",
    "result =  attention_scores.masked_fill(mask==True, -torch.inf)\n",
    "# print(result)\n",
    "attn_weights = torch.softmax(result / key.shape[-1]**0.5, dim=1)\n",
    "# print(attn_weights)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "attn_weights = dropout(attn_weights)\n",
    "# print(attn_weights)\n",
    "context_vectors = torch.matmul(attn_weights,key)\n",
    "print(context_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MULTI-HEAD ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 6])\n",
      "torch.Size([2, 3, 3, 3])\n",
      "torch.Size([2, 3, 3, 2])\n",
      "torch.Size([2, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor(\n",
    "   [[[0.43, 0.15, 0.89, 0.55, 0.87, 0.66],  # Row 1\n",
    "     [0.57, 0.85, 0.64, 0.22, 0.58, 0.33],  # Row 2\n",
    "     [0.77, 0.25, 0.10, 0.05, 0.80, 0.55]] , # Row 3\n",
    "     [[0.10, 0.55, 0.87, 0.66, 0.43, 0.15],  # Row 4\n",
    "     [0.22, 0.58, 0.33, 0.05, 0.85, 0.85],  # Row 5\n",
    "     [0.10, 0.55, 0.8, 0.05, 0.85, 0.85]]] # Row 6\n",
    ")\n",
    "\n",
    "print(inputs.shape)\n",
    "\n",
    "dim_of_weight_matrix = inputs.shape[-1]\n",
    "num_heads = 3\n",
    "head_dim =  int(dim_of_weight_matrix/num_heads)\n",
    "weights_query = torch.nn.Linear(dim_of_weight_matrix, dim_of_weight_matrix,bias = False)\n",
    "weights_value = torch.nn.Linear(dim_of_weight_matrix, dim_of_weight_matrix,bias = False)\n",
    "weights_key = torch.nn.Linear(dim_of_weight_matrix, dim_of_weight_matrix,bias = False)\n",
    "query = weights_query(inputs)\n",
    "key = weights_key(inputs)\n",
    "value = weights_value(inputs)\n",
    "query =  torch.reshape(query,(inputs.shape[0], inputs.shape[1], num_heads,head_dim))\n",
    "key =  torch.reshape(key,(inputs.shape[0], inputs.shape[1], num_heads,head_dim))\n",
    "value = torch.reshape(value,(inputs.shape[0], inputs.shape[1], num_heads,head_dim))\n",
    "query = query.transpose(1,2)\n",
    "key = key.transpose(1,2)\n",
    "value = value.transpose(1,2)\n",
    "multi_head_attention_scores = torch.matmul(query, key.transpose(2,3))\n",
    "mask = torch.triu(torch.ones(multi_head_attention_scores.shape),diagonal=1) ==1\n",
    "result =  multi_head_attention_scores.masked_fill(mask==True, -torch.inf)\n",
    "multi_head_attn_weights = torch.softmax(result / key.shape[-1]**0.5, dim=-1)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "multi_head_attn_weights = dropout(multi_head_attn_weights)\n",
    "\n",
    "print(multi_head_attn_weights.shape)\n",
    "multi_head_context_vectors = torch.matmul(multi_head_attn_weights,value).transpose(1,2)\n",
    "print(multi_head_context_vectors.shape)\n",
    "multi_head_context_vectors = torch.reshape(multi_head_context_vectors,(inputs.shape[0],inputs.shape[1],dim_of_weight_matrix))\n",
    "print(multi_head_context_vectors.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
