{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELF ATTENTION MECHANISM(WITHOUT TRAINABLE WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as f\n",
    "import math\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalizing the attention weights\n",
    "attention_weights = torch.matmul(inputs, inputs.T)\n",
    "attention_weights = f.softmax(attention_weights)\n",
    "# Context Vectors \n",
    "context_vectors = torch.matmul(attention_weights, inputs)\n",
    "context_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELF ATTENTION MECHANISM WITH TRAINABLE WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_of_weight_matrix = inputs.shape[-1]\n",
    "\n",
    "# Initialize weights and queries of the query, value and key matrices\n",
    "torch.manual_seed(123)\n",
    "weights_query = torch.nn.Parameter(torch.randn(dim_of_weight_matrix, dim_of_weight_matrix)) \n",
    "# We can use nn.Linear instead of nn.parameter because they can add bias and good sophisicated weight inititalisation scheme\n",
    "weights_value = torch.nn.Parameter(torch.randn(dim_of_weight_matrix, dim_of_weight_matrix))\n",
    "weights_key = torch.nn.Parameter(torch.randn(dim_of_weight_matrix, dim_of_weight_matrix))\n",
    "\n",
    "# Initialize queries, values and keys\n",
    "query = torch.matmul(inputs,weights_query)\n",
    "key = torch.matmul(inputs,weights_key)\n",
    "value = torch.matmul(inputs,weights_value)\n",
    "\n",
    "# Calculate the attention scores\n",
    "attention_scores = torch.matmul(query,key.T)\n",
    "attention_scores1 = attention_scores/key.shape[-1]**0.5\n",
    "attention_scores1 = f.softmax(attention_scores1)\n",
    "\n",
    "# Calculate the context vectors\n",
    "context_vectors = torch.matmul(attention_scores1,key)\n",
    "context_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MASKED SELF ATTENTION WITH DROPOUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_of_weight_matrix = inputs.shape[-1]\n",
    "print(dim_of_weight_matrix)\n",
    "torch.seed()\n",
    "weights_query = torch.nn.Linear(dim_of_weight_matrix, dim_of_weight_matrix,bias = False)\n",
    "weights_value = torch.nn.Linear(dim_of_weight_matrix, dim_of_weight_matrix,bias = False)\n",
    "weights_key = torch.nn.Linear(dim_of_weight_matrix, dim_of_weight_matrix,bias = False)\n",
    "query = weights_query(inputs)\n",
    "key = weights_key(inputs)\n",
    "value = weights_value(inputs)\n",
    "attention_scores = torch.matmul(query,key.T)\n",
    "# print(attention_scores)\n",
    "mask = torch.triu(torch.ones(attention_scores.shape[0], attention_scores.shape[0]),diagonal=1) ==1\n",
    "# print(mask)\n",
    "result =  attention_scores.masked_fill(mask==True, -torch.inf)\n",
    "# print(result)\n",
    "attn_weights = torch.softmax(result / key.shape[-1]**0.5, dim=1)\n",
    "# print(attn_weights)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "attn_weights = dropout(attn_weights)\n",
    "# print(attn_weights)\n",
    "context_vectors = torch.matmul(attn_weights,key)\n",
    "print(context_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MULTI-HEAD ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "   [[[0.43, 0.15, 0.89, 0.55, 0.87, 0.66],  # Row 1\n",
    "     [0.57, 0.85, 0.64, 0.22, 0.58, 0.33],  # Row 2\n",
    "     [0.77, 0.25, 0.10, 0.05, 0.80, 0.55]] , # Row 3\n",
    "     [[0.10, 0.55, 0.87, 0.66, 0.43, 0.15],  # Row 4\n",
    "     [0.22, 0.58, 0.33, 0.05, 0.85, 0.85],  # Row 5\n",
    "     [0.10, 0.55, 0.8, 0.05, 0.85, 0.85]]] # Row 6\n",
    ")\n",
    "\n",
    "print(inputs.shape)\n",
    "\n",
    "dim_of_weight_matrix = inputs.shape[-1]\n",
    "num_heads = 3\n",
    "head_dim =  int(dim_of_weight_matrix/num_heads)\n",
    "weights_query = torch.nn.Linear(dim_of_weight_matrix, dim_of_weight_matrix,bias = False)\n",
    "weights_value = torch.nn.Linear(dim_of_weight_matrix, dim_of_weight_matrix,bias = False)\n",
    "weights_key = torch.nn.Linear(dim_of_weight_matrix, dim_of_weight_matrix,bias = False)\n",
    "query = weights_query(inputs)\n",
    "key = weights_key(inputs)\n",
    "value = weights_value(inputs)\n",
    "query =  torch.reshape(query,(inputs.shape[0], inputs.shape[1], num_heads,head_dim))\n",
    "key =  torch.reshape(key,(inputs.shape[0], inputs.shape[1], num_heads,head_dim))\n",
    "value = torch.reshape(value,(inputs.shape[0], inputs.shape[1], num_heads,head_dim))\n",
    "query = query.transpose(1,2)\n",
    "key = key.transpose(1,2)\n",
    "value = value.transpose(1,2)\n",
    "multi_head_attention_scores = torch.matmul(query, key.transpose(2,3))\n",
    "mask = torch.triu(torch.ones(multi_head_attention_scores.shape),diagonal=1) ==1\n",
    "result =  multi_head_attention_scores.masked_fill(mask==True, -torch.inf)\n",
    "multi_head_attn_weights = torch.softmax(result / key.shape[-1]**0.5, dim=-1)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "multi_head_attn_weights = dropout(multi_head_attn_weights)\n",
    "\n",
    "print(multi_head_attn_weights.shape)\n",
    "multi_head_context_vectors = torch.matmul(multi_head_attn_weights,value).transpose(1,2)\n",
    "print(multi_head_context_vectors.shape)\n",
    "multi_head_context_vectors = torch.reshape(multi_head_context_vectors,(inputs.shape[0],inputs.shape[1],dim_of_weight_matrix))\n",
    "print(multi_head_context_vectors.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPLEMENTING A MULTI HEAD ATTENTION CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, dim_model, num_heads, bias, model_dropout):\n",
    "        super().__init__()\n",
    "        self.dim_model = dim_model\n",
    "        self.bias = bias\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim_model // num_heads\n",
    "        self.weights_query = torch.nn.Linear(self.dim_model, self.dim_model, bias=self.bias)\n",
    "        self.weights_value = torch.nn.Linear(self.dim_model, self.dim_model, bias=self.bias)\n",
    "        self.weights_key = torch.nn.Linear(self.dim_model, self.dim_model, bias=self.bias)\n",
    "        self.dropout = torch.nn.Dropout(model_dropout)\n",
    "    def forward(self,inputs):\n",
    "        query = self.weights_query(inputs)\n",
    "        key = self.weights_key(inputs)\n",
    "        value = self.weights_value(inputs)\n",
    "        query =  torch.reshape(query,(inputs.shape[0], inputs.shape[1], self.num_heads,self.head_dim))\n",
    "        key =  torch.reshape(key,(inputs.shape[0], inputs.shape[1], self.num_heads,self.head_dim))\n",
    "        value = torch.reshape(value,(inputs.shape[0], inputs.shape[1], self.num_heads,self.head_dim))\n",
    "        query = query.transpose(1,2)\n",
    "        key = key.transpose(1,2)\n",
    "        value = value.transpose(1,2)\n",
    "        multi_head_attention_scores = torch.matmul(query, key.transpose(2,3))\n",
    "        mask = torch.triu(torch.ones(multi_head_attention_scores.shape),diagonal=1) ==1\n",
    "        result =  multi_head_attention_scores.masked_fill(mask==True, -torch.inf)\n",
    "        multi_head_attn_weights = torch.softmax(result / key.shape[-1]**0.5, dim=-1)\n",
    "        dropout = torch.nn.Dropout(0.5)\n",
    "        multi_head_attn_weights = dropout(multi_head_attn_weights)\n",
    "        multi_head_context_vectors = torch.matmul(multi_head_attn_weights,value).transpose(1,2)\n",
    "        multi_head_context_vectors = torch.reshape(multi_head_context_vectors,(inputs.shape[0],inputs.shape[1],self.dim_model))\n",
    "        return multi_head_context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "   [[[0.43, 0.15, 0.89, 0.55, 0.87, 0.66],  # Row 1\n",
    "     [0.57, 0.85, 0.64, 0.22, 0.58, 0.33],  # Row 2\n",
    "     [0.77, 0.25, 0.10, 0.05, 0.80, 0.55]] , # Row 3\n",
    "     [[0.10, 0.55, 0.87, 0.66, 0.43, 0.15],  # Row 4\n",
    "     [0.22, 0.58, 0.33, 0.05, 0.85, 0.85],  # Row 5\n",
    "     [0.10, 0.55, 0.8, 0.05, 0.85, 0.85]]] # Row 6\n",
    ")\n",
    "multi_head = MultiHeadAttention(inputs.shape[-1],3,False,0.1)\n",
    "result = multi_head.forward(inputs)\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LAYER NORMALISATION GELU ACTIVATION AND FEED FORWARD NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
    "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]])\n",
    "mean = tensor.mean( dim =-1, keepdim=True)\n",
    "var = tensor.var( dim =-1, keepdim=True)\n",
    "\n",
    "normalized_tensor = (tensor - mean) / torch.sqrt(var + 1e-5)\n",
    "print(mean, var)\n",
    "print(normalized_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class layer_normalisation(torch.nn.Module):\n",
    "    def __init__(self, dim_model):\n",
    "        super().__init__()\n",
    "        self.scale = torch.nn.Parameter(torch.ones(dim_model))\n",
    "        self.shift = torch.nn.Parameter(torch.zeros(dim_model))\n",
    "        self.eps = 1e-5\n",
    "    def forward(self, inputs):\n",
    "        mean = inputs.mean( dim =-1, keepdim=True)\n",
    "        var = inputs.var( dim =-1, keepdim=True)\n",
    "        normalized_inputs = (inputs - mean) / torch.sqrt(var + self.eps)\n",
    "        normalized_inputs = self.scale * normalized_inputs + self.shift\n",
    "        return normalized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
    "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]])\n",
    "layer_norm = layer_normalisation(6)\n",
    "out_ln = layer_norm.forward(tensor)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(mean, var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeLU(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "     super().__init__()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "       return 0.5 * inputs * (1 + torch.tanh(torch.sqrt(2 / math.pi) * (inputs + 0.044715 * torch.pow(inputs, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class feed_forward(torch.nn.Module):\n",
    "    def __init__(self,dim_model):\n",
    "        super().__init__()\n",
    "        self.layer = torch.nn.Sequential(torch.nn.Linear(dim_model,4*dim_model),GeLU(),\n",
    "                                         torch.nn.Linear(4*dim_model,dim_model))\n",
    "    def forward(self, inputs):\n",
    "        return self.layer(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coding Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, dim_model, num_heads, model_dropout):\n",
    "        super().__init__()\n",
    "        self.layer_norm = layer_normalisation(dim_model)\n",
    "        self.layer_norm2 = layer_normalisation(dim_model)\n",
    "        self.attention = MultiHeadAttention(dim_model, num_heads, False, model_dropout)\n",
    "        self.feed_forward = feed_forward(dim_model)\n",
    "        self.dropout = torch.nn.Dropout(model_dropout)\n",
    "    def forward(self, inputs):\n",
    "        shortcut_connection = inputs\n",
    "        layer_norm_output = self.layer_norm(inputs)\n",
    "        attention_output = self.attention(layer_norm_output)\n",
    "        attention_output = self.dropout(attention_output)\n",
    "        attention_output += shortcut_connection\n",
    "        shortcut_connection = attention_output\n",
    "        layer_norm_output2 = self.layer_norm2(attention_output)\n",
    "        feed_forward_output = self.feed_forward(layer_norm_output2)\n",
    "        feed_forward_output = self.dropout(feed_forward_output)\n",
    "        feed_forward_output += shortcut_connection\n",
    "        return feed_forward_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT ARCHITECTURE -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gpt2_architecture(torch.nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.token_embedding = torch.nn.Embedding(config[\"vocab_size\"],config[\"emb_dim\"])\n",
    "        self.positional_embedding = torch.nn.Embedding(config['context_length'],config[\"emb_dim\"])\n",
    "        self.droput = torch.nn.Dropout(config['drop_rate'])\n",
    "        self.transformer = torch.nn.Sequential(*[Transformer(config['emb_dim'],config['n_heads'],config['drop_rate']) for i in range(config['n_layers'])])\n",
    "        self.final_layer_norm = layer_normalisation(config['emb_dim'])\n",
    "        self.output_layer = torch.nn.Linear(config[\"emb_dim\"],config[\"vocab_size\"])\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x = self.token_embedding(inputs)\n",
    "        x = x + self.positional_embedding(x)\n",
    "        x = self.droput(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.final_layer_norm(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "inputs = torch.tensor([[6109, 3626, 6100,  345]])\n",
    "torch.manual_seed(123)\n",
    "model = gpt2_architecture(GPT_CONFIG_124M)\n",
    "out = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[6109, 3626, 6100,  345]])\n",
    "print(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
